---
title: "HW7 markdown"
output: html_document
---

Setup includes clearing the environment, setting the seed to 1, and saving data set to variable 'dat'

```{r}
library(rpart)
rm(list = ls())
set.seed(1)
dat <- read.table('uscrime.txt', stringsAsFactors=FALSE, header=TRUE)
```

Head of dat
```{r}
head(dat)
```

Problem 10.1
Using the same crime data set uscrime.txt as in Questions 8.2 and 9.1, find the best model you can using
a regression tree model
```{r}
formula <- Crime~M + So + Ed + Po1 + Po2 + LF + M.F + Pop + NW + U1 + U2 + Wealth + Ineq + Prob + Time
m1 <- rpart(formula, method="anova", data=dat)

printcp(m1) # display the results

post(m1, file = "", 
   title = "Regression Tree for USCrimes ")
```

The regression tree using rpart splits the uscrimes data set into four sub sets using the factors P01, Pop and NW. 
The modeling is predicting the crimes to be 550.5 (far left) Po1 is less than 7.65 and Pop is less than 22.5. Similarly, it predict crime amount to be 799.55 when Pop is greater than 22.5.

From the plotted tree we can derive the rest of the model prediction for the different branches of conditions.

```{r}
library(randomForest)
fit <- randomForest(formula,data=dat)
print(fit) # view results 
importance(fit) # importance of each predictor
```

When using a random forest model, random forest gives most importance to Po1.

Problem 10.2
Describe a situation or problem from your job, everyday life, current events, etc., for which a logistic regression model would be appropriate. List some (up to 5) predictors that you might use.

I think logistic regression can help with answering the question whether a new driver will have one or more car accidents within the first year of driving.
Some of the predictors I'll use are
- Age of new driver
- Driving Education 
- Hours of Driving practice
- Family members with driving experience
- Expected mileage of commuting for the year


Problem 10.3.1
Using the GermanCredit data set germancredit.txt from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german / (description at http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29 ), use logistic regression to find a good predictive model for whether credit applicants are good credit risks or not. Show your model (factors used and their coefficients), the software output, and the quality of fit. You can use the glm function in R. To get a logistic regression (logit) model on data where the response is either zero or one, use family=binomial(link=”logit”) in your glm function call.
```{r}
germanCredit <- read.table('germancredit.txt', stringsAsFactors=FALSE, header=FALSE)
head(germanCredit)
## Adding headers
colnames(germanCredit) <- c("CheckingAccountStatus","DurationMonths","CreditHistory",
                            "Purpose","CreditAmount","SavingAccountBonds","PresentEmploymentSince",
                            "InstallmentRatePercentageOfDisposableIncome","PersonalStatusAndSex",
                            "OtherDebtorsGurantors","PresentResidenceSince","Property",
                            "AgeInYears","InstallmentPlans","Housing","ExistingCreditsAtBank",
                            "Job","Dependents","Telephone","ForeignWorker","CreditClassification")

## Changing CreditClassification from 1-2 (Good-Bad) to 0-1(Good-Bad)
germanCredit$CreditClassification[germanCredit$CreditClassification == 1] <- 0
germanCredit$CreditClassification[germanCredit$CreditClassification == 2] <- 1
head(germanCredit)

## splitting data
train <- germanCredit[1:500,]
test <- germanCredit[501:1000,]
formula2 <- CreditClassification ~ CheckingAccountStatus + DurationMonths + CreditHistory +Purpose + CreditAmount + SavingAccountBonds + PresentEmploymentSince + InstallmentRatePercentageOfDisposableIncome + PersonalStatusAndSex + OtherDebtorsGurantors + PresentResidenceSince + Property + AgeInYears + InstallmentPlans + Housing + ExistingCreditsAtBank + Job + Dependents + Telephone + ForeignWorker

logRegModel <- glm(formula=formula2, family=binomial(link="logit"), data=germanCredit)
summary(logRegModel)
```
It appears that InstallmentRatePercentageOfDisposableIncome, SavingAccountBondsA65 (no saving account), PurposeA43 (radio/television), PurposeA41 (used car), CheckingAccountStatusA14 (No Checking account) are statistically significant with not having checking account having the lowest p-value suggesting an association of not having checking account to credit classification.
```{r}
anova(logRegModel, test="Chisq")

library(pscl)
pR2(logRegModel)
```


Problem 10.3.2
Because the model gives a result between 0 and 1, it requires setting a threshold probability to separate between “good” and “bad” answers. In this data set, they estimate that incorrectly identifying a bad customer as good, is 5 times worse than incorrectly classifying a good customer as bad. Determine a good threshold probability based on your model.

A good threshold would be 0.1. If .5 is both classification are evenly costly. Using that threshold, it appears the model will only be 0.564 accurate.
```{r}
fitted.results <- predict(logRegModel,newdata=subset(test,select=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)),type='response')
fitted.results <- ifelse(fitted.results > 0.1,1,0)
misClasificError <- mean(fitted.results != test$CreditClassification)
print(paste('Accuracy',1-misClasificError))

library(ROCR)
p <- predict(logRegModel, newdata=subset(test,select=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)), type="response")
pr <- prediction(p, test$CreditClassification)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```


Code Inspiration Source - https://datascienceplus.com/perform-logistic-regression-in-r/